# Config file for autoencoder training

nbars: 4 # Number of bars in the input
nsources : 4 # Number of sources in the input
nchannels: 2 # Number of channels in the input
num_workers_ds : 0 # Number of workers for loading the dataset
dataset_dir: "D:/audio-dataset-v3" # Directory containing the dataset
output_dir: "D:/output" # Directory to save the output
val_count: 100 # Number of validation samples
sample_rate: 48000 # Sample rate of the audio
splice_size: 195072 # Number of frames in each splice. Has to be somewhat carefully chosen. 195072 is around 4 seconds and it turns into 512 vector embeds

down_channels : [32, 64, 128, 128] # Number of channels in each downsampling layer
mid_channels : [128, 128] # Number of channels in each middle layer
down_sample : [2, 2, 4] # Downsampling factors for each downsampling layer
norm_channels: 32 # Number of channels for normalization
num_heads: 4 # Number of attention heads
num_down_layers : 2 # Number of downsampling layers
num_mid_layers : 2 # Number of middle layers
num_up_layers : 2 # Number of upsampling layers
gradient_checkpointing : True # Whether to use gradient checkpointing
kl_mean: True # Whether to use the mean reduction for the KL divergence

seed : 1943 # Random seed
num_workers_dl: 0 # Number of workers for data loading
autoencoder_batch_size: 1 # Batch size for autoencoder training
disc_start: 3 # Start training discriminator after this many steps. Set to a very small number for testing
disc_weight: 0.5 # Weight for the discriminator loss
disc_hidden: 128 # Number of hidden units in the discriminator
kl_weight: 0.5 # Weight for the KL divergence loss
perceptual_weight: 1 # Weight for the perceptual loss
wasserstein_regularizer: 0.1 # Regularizer for the Wasserstein loss. The lambda term in the paper
gen_weight: 1 # Weight for the generator loss
spec_weight: 1 # Weight for the spectrogram loss
epochs: 2 # Number of training epochs
autoencoder_lr: 0.00001 # Learning rate for the autoencoder
autoencoder_acc_steps: 16 # Number of accumulation steps for the autoencoder
save_steps: 512 # Number of steps between saving images
vqvae_autoencoder_ckpt_name: 'vqvae_autoencoder_ckpt.pth' # Checkpoint name for the VQ-VAE autoencoder
run_name: "vqvae-training" # Name of the training run
disc_loss: "wgan" # Type of discriminator loss: "bce", "mse", or "wgan"
recon_loss: "both" # Type of reconstruction loss: "spec", "l2", or "both". "both" is the sum of the two, "l2" is the L2 loss on audio, and "spec" is the l2 loss on the spectrogram
turn_off_checking_steps: 128 # Number of steps after which we turn off all the sanity checks to hopefully speed up training
val_steps: 512 # Number of steps between validations
